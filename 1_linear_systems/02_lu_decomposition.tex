\section{LU and Cholesky Decompositions}

\begin{bigidea}
The LU decomposition of a matrix $A$ (if it exists) records the row operations of Gaussian elimination in a matrix factorization $A=LU$ where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. If $A$ is symmetric positive definite, then the Cholesky decomposition $A = LL^T$ always exists where $L$ is a lower triangular matrix (not unit lower in general).
\end{bigidea}

\begin{definition}
An {\bf elementary matrix} \cite[p.95]{KN} is a matrix $E$ obtained from the identity matrix $I$ by an elementary row operation. There are 3 types:
\begin{enumerate}
\item Switch rows $i$ and $j$. For example:
$$
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \hspace{5mm}
\begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix} \hspace{5mm}
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \end{bmatrix} \hspace{5mm}
\begin{bmatrix} 0 & 0 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 0 & 0 \end{bmatrix}
$$
\item Multiply row $i$ by $c$. For example:
$$
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix} \hspace{5mm}
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} \hspace{5mm}
\left[ \begin{array}{rrrrr} 1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\
0 & -5 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \end{array} \right]
$$
\item Add $c$ times row $i$ to row $j$. For example:
$$
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 4 & 0 & 1 \end{bmatrix} \hspace{5mm}
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 8 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} \hspace{5mm}
\left[ \begin{array}{rrrrr} 1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\
0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & -2 & 0 & 1 \end{array} \right]
$$
\end{enumerate}
\end{definition}

\begin{proposition}
Let $E$ be an elementary matrix. Then matrix multiplication $EA$ applies the corresponding row operation to $A$ \cite[p.96]{KN}.
\end{proposition}

\begin{definition}
A {\bf unit lower triangular matrix} (see \href{https://en.wikipedia.org/wiki/Triangular_matrix}{Wikipedia: Triangular matrix}) is a square matrix with ones on the diagonal and zeros above diagonal. For example:
$$
\begin{bmatrix} 1 & & \\ * & 1 & & \\ * & * & 1 & \\ * & * & * & 1 \end{bmatrix}
$$
An {\bf atomic lower triangular matrix} has nonzero entries below the diagonal in only one column:
$$
L_k =
\begin{bmatrix}
1 & & & & & \\
 & \ddots & & & & \\
 & & 1 & & & \\
 & & \ell_{k+1,k}& 1 & & \\
 & & \vdots & & \ddots & \\
 & & \ell_{m,k} & & & 1 \\
\end{bmatrix}
$$
\end{definition}

\begin{proposition}
\begin{enumerate}
\item Let $A$ be a $m$ by $n$ matrix and let $L_k$ be an atomic lower triangular matrix with nonzero entries in column $k$. Multiplication $L_k A$ adds $\ell_{i,k}$ times row $k$ of $A$ to row $i$ for each $i = k+1,\dots,m$.
\item The inverse of an atomic lower triangular matrix:
$$
L_k =
\begin{bmatrix}
1 & & & & & \\
 & \ddots & & & & \\
 & & 1 & & & \\
 & & \ell_{k+1,k}& 1 & & \\
 & & \vdots & & \ddots & \\
 & & \ell_{m,k} & & & 1 \\
\end{bmatrix}
\hspace{5mm} \Rightarrow \hspace{5mm} 
L_k^{-1} =
\begin{bmatrix}
1 & & & & & \\
 & \ddots & & & & \\
 & & 1 & & & \\
 & & -\ell_{k+1,k}& 1 & & \\
 & & \vdots & & \ddots & \\
 & & -\ell_{m,k} & & & 1 \\
\end{bmatrix}
$$
\item Multiplication of atomic lower triangular matrices:
\begin{align*}
L_1 L_2 \cdots L_{m-1} &=
\begin{bmatrix}
1 & & & & \\
\ell_{2,1} & 1 & & & \\
\ell_{3,1} & & 1 & & \\
\vdots & & & \ddots & \\
\ell_{m,1} & & & & 1 \\
\end{bmatrix}
\begin{bmatrix}
1 & & & & \\
& 1 & & & \\
& \ell_{3,2} & 1 & & \\
& \vdots & & \ddots & \\
& \ell_{m,2} & & & 1 \\
\end{bmatrix}
\dots
\begin{bmatrix}
1 & & & & \\
& 1 & & & \\
& & 1 & & \\
& & & \ddots & \\
& & & \ell_{m,m-1} & 1 \\
\end{bmatrix} \\
&=
\begin{bmatrix}
1 & & & & \\
\ell_{2,1} & 1 & & & \\
\ell_{3,1} & \ell_{3,2} & 1 & & \\
\vdots & \vdots & \ddots & \ddots & \\
\ell_{m,1} & \ell_{m,2} & \cdots & \ell_{m,m-1} & 1 \\
\end{bmatrix}
\end{align*}
\end{enumerate}
\end{proposition}

\begin{theorem}
If $A$ can be reduced by Gaussian elimination to row echelon form without pivoting (that is, without interchanging rows), then
$$A = LU$$
where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. This is called the {\bf LU decomposition} of $A$ \cite[p.68]{MH}. The algorithm is:
\begin{enumerate}
\item[] Let $m$ be the number of rows of $A$ and let $n$ be the number of columns.
\item[] Let $A_1 = A$ and use notation $A_k = \left[ a_{i,j}^{(k)} \right]$.
\item[] {\bf for} $k$ from 1 to $n-1$:
\begin{enumerate}
\item[] Let $L_k$ such that $\ds \ell_{i,k} = - \frac{a^{(k)}_{i,k}}{a^{(k)}_{k,k}}$ and compute $L_k A_k = A_{k+1}$.
\end{enumerate}
\item[] {\bf end}
\item[] $A = LU$ where $L = L_1^{-1} \cdots L_{n-1}^{-1}$ is unit lower triangular and $U = A_n$ is upper triangular
\end{enumerate}
\end{theorem}

\begin{note}
In each step of the LU decomposition algorithm, the operation $L_k A_k$ eliminates entries in $A_k$ below the diagonal in column $k$. The result is $L_{n-1} \cdots L_1 A = U$ where $U = A_n$ is upper triangular and therefore $A = LU$ where $L = L_1^{-1} \cdots L_{n-1}^{-1}$ is unit lower triangular.
\end{note}

\begin{example}
Compute the LU decomposition of
$$
A = \begin{bmatrix} 2 & 1 & 1 \\ 2 & 0 & 2 \\ 4 & 3 & 4 \end{bmatrix}
$$
Start with $A_1 = A$ and compute
\begin{align*}
L_1 A_1 &= \begin{bmatrix} \phantom{-}1 & 0 & 0 \\ -1 & 1 & 0 \\ -2 & 0 & 1 \end{bmatrix}
\begin{bmatrix} 2 & 1 & 1 \\ 2 & 0 & 2 \\ 4 & 3 & 4 \end{bmatrix}
=
\left[ \begin{array}{rrr} 2 & 1 & 1 \\ 0 & -1 & \phantom{+}1 \\ 0 & 1 & 2 \end{array} \right] \\
L_2 A_2 &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 1 \end{bmatrix}
\left[ \begin{array}{rrr} 2 & 1 & 1 \\ 0 & -1 & \phantom{+}1 \\ 0 & 1 & 2 \end{array} \right]
=
\left[ \begin{array}{rrr} 2 & 1 & 1 \\ 0 & -1 & \phantom{+}1 \\ 0 & 0 & 3 \end{array} \right]
\end{align*}
Therefore $A = LU$ where
$$
L = L_1^{-1} L_2^{-1}
%= \begin{bmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 2 & 0 & 1 \end{bmatrix}
%\left[ \begin{array}{rrr}  1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -1 & \phantom{+}1 \end{array} \right]
=
\left[ \begin{array}{rrr} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 2 & -1 & \phantom{+}1 \end{array} \right]
\hspace{10mm}
U = \left[ \begin{array}{rrr} 2 & 1 & 1 \\ 0 & -1 & \phantom{+}1 \\ 0 & 0 & 3 \end{array} \right] \\
$$
\end{example}

\begin{note} Suppose $A$ has an LU decomposition $A = LU$. Applications of LU are:
\begin{enumerate}
\item Solve the system of equations $A \bs{x} = \bs{b}$ by:
\begin{itemize}
\item Solve $L \bs{y} = \bs{b}$ by forward substitution
\item Solve $U \bs{x} = \bs{y}$ by backward substitution
\end{itemize} 
\item $\mathrm{rank}(A) = \mathrm{rank}(U)$
\item $\det(A) = \det(U) =$ product of diagonal entries of $U$
\item Compute $A^{-1}$: use LU decomposition to solve $A \bs{x}_1 = \bs{e}_1, \dots, A \bs{x}_n = \bs{e}_n$ where $\bs{e}_k$ is the $k$th column of the identity $I$ and then $A^{-1} = [\bs{x}_1 \cdots \bs{x}_n]$ (that is, the columns of $A^{-1}$ are given by $\bs{x}_1, \dots, \bs{x}_n$)
\end{enumerate}
\end{note}

\begin{example}
Compute the LU decomposition of
$$
A = \left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 2 & 1 & 5 & 3 \\ 1 & 0 & 0 & 2 \\ 0 & -1 & \phantom{+}1 & \phantom{+}1 \end{array} \right]
$$
Start with $A_1 = A$ and compute
\begin{align*}
L_1 A_1 &=
\left[ \begin{array}{rrrr} \phantom{+}1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\ -2 & 1 & 0 & 0 \\ -1 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{array} \right]
\left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 2 & 1 & 5 & 3 \\ 1 & 0 & 0 & 2 \\ 0 & -1 & \phantom{+}1 & \phantom{+}1 \end{array} \right]
=
\left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & 1 \\ 0 & -1 & 1 & \phantom{+}1 \end{array} \right] \\
L_2 A_2 &=
\left[ \begin{array}{rrrr} \phantom{+}1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \end{array} \right]
\left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & 1 \\ 0 & -1 & 1 & \phantom{+}1 \end{array} \right]
=
\left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & 1 \\ 0 & \phantom{+}0 & 2 & \phantom{+}2 \end{array} \right] \\
L_3 A_3 &=
\left[ \begin{array}{rrrr} \phantom{+}1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 1 & 1 \end{array} \right]
\left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & 1 \\ 0 & \phantom{+}0 & 2 & \phantom{+}2 \end{array} \right]
=
\left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & 1 \\ 0 & \phantom{+}0 & 0 & \phantom{+}3 \end{array} \right]
\end{align*}
And we compute $A = LU$ using properties of atomic lower triangular matrices
$$
L
=
L_1^{-1} L_2^{-1} L_3^{-1}
=
%\left[ \begin{array}{rrrr} 1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\ 2 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{array} \right]
%\left[ \begin{array}{rrrr} 1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & -1 & 0 & 1 \end{array} \right]
%\left[ \begin{array}{rrrr} 1 & \phantom{+}0 & \phantom{+}0 & \phantom{+}0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & -1 & 1 \end{array} \right] \\
%=
\left[ \begin{array}{rrrr} 1 & 0 & 0 & 0 \\ 2 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & -1 & -1 & \phantom{+}1 \end{array} \right]
\hspace{10mm}
U = \left[ \begin{array}{rrrr} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & 1 \\ 0 & \phantom{+}0 & 0 & \phantom{+}3 \end{array} \right]
$$
\end{example}

\begin{theorem}
Let $A$ be an $m$ by $n$ matrix. If $\mathrm{rank}(A) = m$ and $A$ has a LU decomposition $A = LU$ where $L$ is unit lower triangular and $U$ is upper triangular, then the matrices $L$ and $U$ are unique \cite[p.124]{KN}.
\end{theorem}

\begin{definition}
Let $A$ be a square matrix.
\begin{enumerate}
\item $A$ is {\bf symmetric} if $A^T = A$.
\item $A$ is {\bf positive definite} if $\bs{x}^T A \bs{x} > 0$ for all $\bs{x} \not= 0$.
\end{enumerate}
See \cite[p.84]{MH}.
\end{definition}

\begin{example}
If $A$ is invertible, then both $AA^T$ and $A^TA$ are symmetric positive definite.
\end{example}

\begin{theorem}
Let $A$ be a symmetric positive definite matrix. There exists a lower triangular matrix $L$ with positive diagonal entries (not necessarily unit lower triangular) such that
$$
A = L L^T
$$
This is called the {\bf Cholesky decomposition} of $A$ \cite[p.84]{MH}.
\end{theorem}

\begin{proposition}
Let $A$ be a symmetric positive definite matrix. There exists a {\it unit} lower triangular matrix $L$ and a diagonal matrix $D$ with {\it positive} entries such that
$$
A = LDL^T
$$
Furthermore, let $\sqrt{D}$ be the diagonal matrix where the entries are the square roots of the entries in $D$ (that is, $\sqrt{D}^2 = D$). Then
$$
A = (L\sqrt{D})(\sqrt{D}L^T)
$$
is the Cholesky decomposition of $A$.
\end{proposition}

\begin{example}
Computing the Cholesky decomposition requires about half the computations of the LU factorization since we need only compute $L$ or $U$ by Gaussian elimination. Compute the Cholesky decomposition of $A$ where $A = M^TM$ for
$$
M =
\left[ \begin{array}{rrr} 1 & 1 & -1 \\ 0 & -1 & -1 \\ 1 & 1 & 1 \end{array} \right]
\ \ \Rightarrow \ \
A =
\left[ \begin{array}{rrr} 2 & 2 & 0 \\ 2 & 3 & 1 \\ 0 & 1 & 3 \end{array} \right]
$$
We know that $A$ is symmetric positive definite by construction and so we proceed by Gaussian elimination (without scaling or switching rows)
$$
\left[ \begin{array}{rrr} 2 & 2 & 0 \\ 2 & 3 & 1 \\ 0 & 1 & 3 \end{array} \right]
\rightarrow
\left[ \begin{array}{rrr} 2 & 2 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & 3 \end{array} \right]
\rightarrow
\left[ \begin{array}{ccc} 2 & 2 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 2 \end{array} \right]
$$
Factor out the diagonal
$$
\left[ \begin{array}{ccc} 2 & 2 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 2 \end{array} \right]
=
\left[ \begin{array}{ccc} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2 \end{array} \right]
\left[ \begin{array}{ccc} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{array} \right]
$$
Therefore the Cholesky decomposition is $A = LL^T$ where
$$
L = 
\left[ \begin{array}{ccc} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 0 & 1 & 1 \end{array} \right]
\left[ \begin{array}{ccc} \sqrt{2} & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \sqrt{2} \end{array} \right]
=
\left[ \begin{array}{ccc} \sqrt{2} & 0 & 0 \\ \sqrt{2} & 1 & 0 \\ 0 & 1 & \sqrt{2} \end{array} \right]
$$
\end{example}