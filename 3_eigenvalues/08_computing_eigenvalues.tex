\section{Computing Eigenvalues}

\begin{bigidea}
It is not practical to compute eigenvalues of a matrix $A$ by finding roots of the characteristic polynomial $c_A(x)$. Instead, there are several efficient algorithms for numerically approximating eigenvalues without using $c_A(x)$ such as the power method.
\end{bigidea}

\begin{definition}
Let $A$ be a square matrix. An eigenvalue $\lambda$ of $A$ is called a {\bf dominant eigenvalue} \cite[p.441]{KN} if $\lambda$ has multiplicity 1 and $| \lambda | > | \mu |$ for all other eigenvalues $\mu$.
\end{definition}

\begin{definition}
Let $A$ be an $n \times n$ matrix with eigenvalues $\lambda_1,\dots,\lambda_n$ and corresponding eigenvectors $\bs{v}_1,\dots,\bs{v}_n$ with dominant eigenvalue $\lambda_1$. Let $\bs{x}_0$ be any vector which is a linear combination of the eigenvectors of $A$
$$
\bs{x}_0 = c_1 \bs{v}_1 + \cdots + c_n \bs{v}_n
$$
such that $c_1 \not= 0$. Then
$$
A^k \bs{x}_0 = c_1 \lambda_1^k \bs{v}_1 + \cdots + c_n \lambda_n^k \bs{v}_n
$$
and therefore
$$
(1/\lambda_1^k)A^k \bs{x}_0 = c_1 \bs{v}_1 + c_2 (\lambda_2/\lambda_1)^k \bs{v}_2 + \cdots + c_n (\lambda_n/\lambda_1)^k \bs{v}_n
\ \ \rightarrow \ \ c_1 \bs{v}_1 \ \ \text{as } k \to \infty
$$
because each term $| \lambda_i/\lambda_1 | < 1$ and so $\lambda_i/\lambda_1 \to 0$ as $k \to \infty$. This method of approximating $\bs{v}_1$ is called {\bf power iteration} \cite[p.172]{MH} (or the {\bf power method}).
\end{definition}

\begin{note}
The entries in the vector $A^k \bs{x}_0$ may get very large as $k$ increases therefore it is helpful to normalize at each step. The simplest way is to divide by the $\infty$-norm
$$
\bs{x}_{k+1} = \frac{A \bs{x}_k}{\| A \bs{x}_k \|_{\infty}}
$$
This is called {\bf normalized power iteration} \cite[p.174]{MH}. Note that $\| A \bs{x}_k \|_{\infty}$ gives an approximation of $\lambda_1$ at each step.
\end{note}

\begin{example}
Approximate the dominant eigenvalue and eigenvector of the matrix
$$
A = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}
$$
by 4 iterations of the normalized power method. Choose a random starting vector
$$
\bs{x}_0 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
$$
and compute
\begin{align*}
A \bs{x}_0 &=
\begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
&
\bs{x}_1 = \frac{A \bs{x}_0}{\| A \bs{x}_0 \|_{\infty}}
&=
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
\\
A \bs{x}_1 &=
\begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}
&
\bs{x}_2 = \frac{A \bs{x}_1}{\| A \bs{x}_1 \|_{\infty}}
&=
\begin{bmatrix} 1 \\ 1 \\ 0.5 \end{bmatrix}
\\
A \bs{x}_2 &=
\begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 1 \\ 0.5 \end{bmatrix}
=
\begin{bmatrix} 2 \\ 2.5 \\ 1.5 \end{bmatrix}
&
\bs{x}_3 = \frac{A \bs{x}_2}{\| A \bs{x}_2 \|_{\infty}}
&=
\begin{bmatrix} 0.8 \\ 1 \\ 0.6 \end{bmatrix}
\\
A \bs{x}_3 &=
\begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}
\begin{bmatrix} 0.8 \\ 1 \\ 0.6 \end{bmatrix}
=
\begin{bmatrix} 1.8 \\ 2.4 \\ 1.6 \end{bmatrix}
&
\bs{x}_4 = \frac{A \bs{x}_3}{\| A \bs{x}_3 \|_{\infty}}
&=
\begin{bmatrix} 0.75 \\ 1 \\ 0.67 \end{bmatrix}
\end{align*}
Therefore we get approximations
$$
\lambda_1 \approx 2.4
\hspace{10mm}
\bs{v}_1 \approx \begin{bmatrix} 0.75 \\ 1 \\ 0.67 \end{bmatrix}
$$
The actual dominant eigenvector is
$$
\bs{v}_1 = \begin{bmatrix} 1/\sqrt{2} \\ 1 \\ 1/\sqrt{2} \end{bmatrix}
$$
and we verify
$$
\begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}
\begin{bmatrix} 1/\sqrt{2} \\ 1 \\ 1/\sqrt{2} \end{bmatrix}
=
\begin{bmatrix} \frac{1 + \sqrt{2}}{\sqrt{2}} \\ 1 + \sqrt{2} \\ \frac{1 + \sqrt{2}}{\sqrt{2}} \end{bmatrix}
=
(1 + \sqrt{2}) \begin{bmatrix} \frac{1}{\sqrt{2}} \\ 1 \\ \frac{1}{\sqrt{2}} \end{bmatrix}
$$
therefore $\lambda_1 \approx 2.4142$.
\end{example}

\begin{definition}
Let $A$ be an $n \times n$ matrix with eigenvalues $\lambda_1,\dots,\lambda_n$ (in increasing order $\lambda_1 < \lambda_2 < \cdots < \lambda_n$) with corresponding eigenvectors $\bs{v}_1,\dots,\bs{v}_n$. Then $1/\lambda_1,\dots,1/\lambda_n$ are the eigenvalues of $A^{-1}$ (in decreasing order) with corresponding eigenvectors $\bs{v}_1,\dots,\bs{v}_n$. {\bf Inverse iteration} \cite[p.175]{MH} is power iteration applied to $A^{-1}$ to find the dominant eigenvalue $1/\lambda_n$ of $A^{-1}$ (equivalently, the smallest eigenvalue $\lambda_n$ of $A$) with eigenvector $\bs{v}_n$. At each step, solve the system and normalize
$$
\bs{y}_{k+1} = A^{-1} \bs{x}_k \ \ \Rightarrow \ \ A \bs{y}_{k+1} = \bs{x}_k \ \ \Rightarrow \ \ \bs{x}_{k+1} = \frac{\bs{y}_{k+1}}{\| \bs{y}_{k+1} \|_{\infty}}
$$
\end{definition}

\begin{example}
Compute 2 steps of inverse iterations for the matrix
$$
A = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 0 \\ 1 & 0 & 3 \end{bmatrix}
$$
Since we are going to repeatedly solve systems $A \bs{x} = \bs{b}$, we should find the LU decomposition and use forward and backward substitution
\begin{align*}
\left[ \begin{array}{rrr} 1 & 0 & 0 \\ -1 & 1 & 0 \\ -1 & 0 & 1 \end{array} \right]
\begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 0 \\ 1 & 0 & 3 \end{bmatrix}
&=
\left[ \begin{array}{rrr} 1 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & -1 & 2 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 1 \end{array} \right]
\left[ \begin{array}{rrr} 1 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & -1 & 2 \end{array} \right]
&=
\left[ \begin{array}{rrr} 1 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & \phantom{+}0 & 1 \end{array} \right] \\
\end{align*}
Therefore
$$
A = LU =
\left[ \begin{array}{rrr} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & -1 & \phantom{+}1 \end{array} \right]
\left[ \begin{array}{rrr} 1 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & \phantom{+}0 & 1 \end{array} \right]
$$
and in fact we see that $A$ is positive definite and this is the Cholesky decomposition. Choose a random starting vector
$$
\bs{x}_0 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
$$
and compute
\begin{align*}
\left[ \begin{array}{rrr} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & -1 & \phantom{+}1 \end{array} \right] \bs{z}_1 &=  \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
& \bs{z}_1 &= \left[ \begin{array}{r} 1 \\ -1 \\ -2 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & \phantom{+}0 & 1 \end{array} \right] \bs{y}_1 &= \left[ \begin{array}{r} 1 \\ -1 \\ -2 \end{array} \right]
& \bs{y}_1 &= \left[ \begin{array}{r} 6 \\ -3 \\ -2 \end{array} \right] 
& \bs{x}_1 &= \left[ \begin{array}{c} 1 \\ -1/2 \\ -1/3 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & -1 & \phantom{+}1 \end{array} \right] \bs{z}_2 &= \left[ \begin{array}{c} 1 \\ -1/2 \\ -1/3 \end{array} \right]
& \bs{z}_2 &= \left[ \begin{array}{c} 1 \\ -3/2 \\ -17/6 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & \phantom{+}0 & 1 \end{array} \right] \bs{y}_2 &= \left[ \begin{array}{c} 1 \\ -3/2 \\ -17/6 \end{array} \right]
& \bs{y}_2 &= \left[ \begin{array}{c} 49/6 \\ -13/3 \\ -17/6 \end{array} \right]
& \bs{x}_2 &= \left[ \begin{array}{c} 1 \\ -26/49 \\ -17/49 \end{array} \right]
\end{align*}
Our approximation of the eigenvector of $A$ corresponding to the smallest eigenvalue is
$$
\bs{v} = \left[ \begin{array}{c} 1 \\ -26/49 \\ -17/49 \end{array} \right]
\approx
\left[ \begin{array}{r} 1.00 \\ -0.53 \\ -0.35 \end{array} \right]
$$
with eigenvalue $\lambda \approx 0.12$ given by
$$
\begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 0 \\ 1 & 0 & 3 \end{bmatrix}
\left[ \begin{array}{r} 1.00 \\ -0.53 \\ -0.35 \end{array} \right]
=
\left[ \begin{array}{r} 0.12 \\ -0.06 \\ -0.05 \end{array} \right]
=
0.12 \left[ \begin{array}{r} 1.00 \\ -0.50 \\ -0.42 \end{array} \right]
$$
The actual eigenvector is approximately
$$
\bs{v} \approx \left[ \begin{array}{r} 1.000 \\ -0.532 \\ -0.347 \end{array} \right]
$$
with eigenvalue
$$
\lambda \approx 0.12061476
$$
\end{example}